{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nimport os\nimport numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nwarnings.simplefilter(\"ignore\")\n%matplotlib inline\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load the training dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(os.path.join('..', 'input', 'train.csv'), index_col=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore the training dataset"},{"metadata":{},"cell_type":"markdown","source":"## Print row and types"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check and remove duplicated rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is 7 duplicated rows, let's remove them"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.drop_duplicates()\ndf_train.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check for null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check for outliers"},{"metadata":{},"cell_type":"markdown","source":"First let's describe the dataframe with pretty printed columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"describe = df_train.describe()\n\ndef seconds_to_pretty(seconds):\n    seconds = int(float(seconds))\n    return '{} h {} m {} s ({} s)'.format(seconds // 3600, (seconds % 3600) // 60, (seconds % 3600) % 60, seconds)\n\ndescribe = describe.drop('count') # to disable scientific notation\ndescribe['trip_duration'] = describe['trip_duration'].apply(str)\ndescribe['trip_duration'].loc[['mean', 'min', '25%', '50%', '75%', 'max']] = describe['trip_duration'].loc[['mean', 'min', '25%', '50%', '75%', 'max']].apply(seconds_to_pretty)\ndescribe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see here that some values seem to be outliers:\n* max trip_duration seems a way too long\n* min trip_duration seems too short\n* min passenger_count is 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def pretty_print_max_trip_duration(df):   \n#     max_trip_duration = df_train['trip_duration'].max()\n#     print('Max duration trip: ({} sec) -> {} hours {} minutes {} secondes'.format(max_trip_duration, max_trip_duration // 3600, (max_trip_duration % 3600) // 60, (max_trip_duration % 3600) % 60))\n\n# pretty_print_max_trip_duration(df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's make a boxplot to better outliers visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's make a boxplot to better trip_duration outliers visualization\n# we need to see if there is any correlation between trip_duration and store_and_fwd_flag\ndef boxplot_trip_duration(df, ylim=None):\n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))\n    plt.subplots_adjust(wspace=1)\n    if ylim:\n        ax.set_ylim(ylim)\n    sns.boxplot(data=df_train, y='trip_duration', x='store_and_fwd_flag', fliersize=5, ax=ax).axes.set_title(label='Trip duration outliers visualization \\n (store_and_fwd_flag = N)', fontsize=16, pad=25)\n#     sns.boxplot(y=df_train[df_train['store_and_fwd_flag'] == 'N']['trip_duration'], fliersize=5, ax=ax1).axes.set_title(label='Trip duration outliers visualization \\n (store_and_fwd_flag = N)', fontsize=16, pad=25)\n#     sns.boxplot(y=df_train[df_train['store_and_fwd_flag'] == 'Y']['trip_duration'], fliersize=5, ax=ax2).axes.set_title(label='Trip duration outliers visualization \\n (store_and_fwd_flag = Y)', fontsize=16, pad=25)\n    \nboxplot_trip_duration(df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The higer trip_duration outliers seem to be present when store_and_fwd_flag = N, this is probably due to the vehicle did not have a connection to the server during a long time"},{"metadata":{"trusted":true},"cell_type":"code","source":"with_n = len(df_train[df_train['store_and_fwd_flag'] == 'N'].index)\nwith_y = len(df_train[df_train['store_and_fwd_flag'] == 'Y'].index)\n\nlen(df_train[df_train['store_and_fwd_flag'] == 'N'].index), len(df_train[df_train['store_and_fwd_flag'] == 'Y'].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a majority of rows with store_and_fwd_flag = no, so we can't afford to remove them"},{"metadata":{"trusted":true},"cell_type":"code","source":"seconds_to_pretty(df_train[df_train['store_and_fwd_flag'] == 'N']['trip_duration'].max()), seconds_to_pretty(df_train[df_train['store_and_fwd_flag'] == 'Y']['trip_duration'].max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2 h 46 m 24 s may seem long for a trip to New York, but in doubt we will keep it"},{"metadata":{},"cell_type":"markdown","source":"Let's zoom and the small values"},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot_trip_duration(df_train, [0, df_train[df_train['store_and_fwd_flag'] == 'N']['trip_duration'].mean()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have to delete the values near 0"},{"metadata":{},"cell_type":"markdown","source":"Let's see the passenger_count distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.distplot(df_train['passenger_count'], hist=False, rug=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a majority of trips with 1 passanger"},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the pickup positions\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.jointplot(data=df_train, x=\"pickup_latitude\", y=\"pickup_longitude\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the dropoff positions"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.jointplot(data=df_train, x=\"dropoff_latitude\", y=\"dropoff_longitude\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is some outilers positions"},{"metadata":{},"cell_type":"markdown","source":"# Load the test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(os.path.join('..', 'input', 'test.csv'), index_col=0)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_test.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compare train and test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe().drop('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.describe().drop('count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Latitudes and longitudes seems pretty similar"},{"metadata":{},"cell_type":"markdown","source":"passenger_count seems pretty similar too, so we don't need to filter them"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.distplot(df_train['passenger_count'], hist=False, rug=True).set_title(label='Train dataset', fontsize=16, pad=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.distplot(df_test['passenger_count'], hist=False, rug=True).set_title(label='Test dataset', fontsize=16, pad=25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution is pretty similar too"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.jointplot(data=df_test, x=\"pickup_latitude\", y=\"pickup_longitude\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.jointplot(data=df_test, x=\"dropoff_latitude\", y=\"dropoff_longitude\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The test dataset seems to have more extreme coordinates"},{"metadata":{},"cell_type":"markdown","source":"## Fix outliers"},{"metadata":{},"cell_type":"markdown","source":"In first, let's delete all the rows with not stored and forwarded trip and trip_duration > max of trip_duration of rows with stored and forwarded trip"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_trip_duration_flag_y = df_train[df_train['store_and_fwd_flag'] == 'Y']['trip_duration'].max()\nrow_count_before_deletion = len(df_train.index)\nprint('{} lines before deletion'.format(row_count_before_deletion))\ndf_train = df_train[~((df_train['store_and_fwd_flag'] == 'N') & (df_train['trip_duration'] > max_trip_duration_flag_y))]\nrow_count_after_deletion = len(df_train.index)\nprint('{} lines after deletion'.format(row_count_after_deletion))\nprint('{} lines deleted'.format(row_count_before_deletion - row_count_after_deletion))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have deleted 2122 rows on 1458637 row, that's ok\n"},{"metadata":{},"cell_type":"markdown","source":"let's check the outliers visualization now"},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot_trip_duration(df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seem to be still extreme values on Y column"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.distplot(df_train[df_train['store_and_fwd_flag'] == 'Y']['trip_duration'], hist=False, rug=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[((df_train['store_and_fwd_flag'] == 'Y') & (df_train['trip_duration'] > 6500))]['trip_duration'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"~~I think we need to remove all trip durations > 6500~~, i think this method overfit the model :/"},{"metadata":{"trusted":true},"cell_type":"code","source":"# row_count_before_deletion = len(df_train.index)\n# print('{} lines before deletion'.format(row_count_before_deletion))\n# df_train = df_train[df_train['trip_duration'] <= 6500]\n# row_count_after_deletion = len(df_train.index)\n# print('{} lines after deletion'.format(row_count_after_deletion))\n# print('{} lines deleted'.format(row_count_before_deletion - row_count_after_deletion))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# _ = sns.distplot(df_train['trip_duration'], hist=False, rug=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, we are good for the to long trip durations"},{"metadata":{},"cell_type":"markdown","source":"1. Now let's remove the trip durations <= 10 sec, a trip duration <= 10 sec makes no sens"},{"metadata":{"trusted":true},"cell_type":"code","source":"row_count_before_deletion = len(df_train.index)\nprint('{} lines before deletion'.format(row_count_before_deletion))\ndf_train = df_train[df_train['trip_duration'] >= 10]\nrow_count_after_deletion = len(df_train.index)\nprint('{} lines after deletion'.format(row_count_after_deletion))\nprint('{} lines deleted'.format(row_count_before_deletion - row_count_after_deletion))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"~~Now lets remove trips with passenger_count = 0~~"},{"metadata":{},"cell_type":"markdown","source":"We dont have to remove passenger_count = 0 because the test dataset have too"},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train[df_train['passenger_count'] < 1]['passenger_count'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# row_count_before_deletion = len(df_train.index)\n# print('{} lines before deletion'.format(row_count_before_deletion))\n# df_train = df_train[df_train['passenger_count'] >= 1]\n# row_count_after_deletion = len(df_train.index)\n# print('{} lines after deletion'.format(row_count_after_deletion))\n# print('{} lines deleted'.format(row_count_before_deletion - row_count_after_deletion))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scale the train dataset to the test dataset"},{"metadata":{},"cell_type":"markdown","source":"Determine max and min lat, long from the test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# max_test_pickup_latitude = df_test['pickup_latitude'].max()\n# min_test_pickup_latitude = df_test['pickup_latitude'].min()\n\n# max_test_pickup_longitude = df_test['pickup_longitude'].max()\n# min_test_pickup_longitude = df_test['pickup_longitude'].min()\n\n\n# max_test_dropoff_latitude = df_test['dropoff_latitude'].max()\n# min_test_dropoff_latitude = df_test['dropoff_latitude'].min()\n\n# max_test_dropoff_longitude = df_test['dropoff_longitude'].max()\n# min_test_dropoff_longitude = df_test['dropoff_longitude'].min()\n\n# (max_test_pickup_latitude, min_test_pickup_latitude, '---',\n#  max_test_pickup_longitude, min_test_pickup_longitude, '---',\n#  max_test_dropoff_latitude, min_test_dropoff_latitude, '---',\n#  max_test_dropoff_longitude, min_test_dropoff_longitude)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# row_count_before_deletion = len(df_train.index)\n# print('{} lines before deletion'.format(row_count_before_deletion))\n\n# df_train['pickup_latitude'] = df_train[df_train['pickup_latitude'] <= max_test_pickup_latitude]['pickup_latitude']\n# df_train['pickup_latitude'] = df_train[df_train['pickup_latitude'] >= min_test_pickup_latitude]['pickup_latitude']\n\n# df_train['pickup_longitude'] = df_train[df_train['pickup_longitude'] <= max_test_pickup_longitude]['pickup_longitude']\n# df_train['pickup_longitude'] = df_train[df_train['pickup_longitude'] >= min_test_pickup_longitude]['pickup_longitude']\n\n\n# df_train['dropoff_latitude'] = df_train[df_train['dropoff_latitude'] <= max_test_dropoff_latitude]['dropoff_latitude']\n# df_train['dropoff_latitude'] = df_train[df_train['dropoff_latitude'] >= min_test_dropoff_latitude]['dropoff_latitude']\n\n# df_train['dropoff_longitude'] = df_train[df_train['dropoff_longitude'] <= max_test_dropoff_longitude]['dropoff_longitude']\n# df_train['dropoff_longitude'] = df_train[df_train['dropoff_longitude'] >= min_test_dropoff_longitude]['dropoff_longitude']\n\n# row_count_after_deletion = len(df_train.index)\n# print('{} lines after deletion'.format(row_count_after_deletion))\n# print('{} lines deleted'.format(row_count_before_deletion - row_count_after_deletion))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define data manipulation functions and variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"from geopy.distance import geodesic\n\ndef create_datetime_based_columns(df):\n    df['datetime'] =  pd.to_datetime(df['pickup_datetime'])\n    \n    df['year'] = df['datetime'].dt.year # year seems to help\n    df['month'] = df['datetime'].dt.month # month doesn't help at all\n    df['day'] = df['datetime'].dt.day\n    df['dayofweek'] = df['datetime'].dt.weekday\n    df['hour'] = df['datetime'].dt.hour\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define filtering/split functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def filter_feature_columns(df):\n    selected_columns = []\n    selected_columns = ['pickup_longitude', 'pickup_latitude'] \n    selected_columns += ['dropoff_longitude', 'dropoff_latitude']\n    selected_columns += ['month', 'dayofweek', 'hour']\n#     selected_columns += ['store_and_fwd_flag']\n#     selected_columns += ['lat_distance', 'long_distance']\n    return df[selected_columns]\n\ndef filter_target_column(df):\n    return df['trip_duration']\n    \n\ndef filter_split_dataset(df):\n    X = filter_feature_columns(df)\n    y = filter_target_column(df)\n    \n    return X, y\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's measure loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_copy = df_train.copy() # we have to work on a copy of df_train to be able to repeat the operations from 0 without having to reload the dataset\ndf_train_copy = create_datetime_based_columns(df_train_copy)\n# tmp_df_train = normalize_store_and_fwd_flag(tmp_df_train)\n# df_train_copy = create_distance_column(df_train_copy)\n\ndf_train_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train = filter_split_dataset(df_train_copy)\nX_train.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import RandomizedSearchCV # test des combinaisons aléatoirent de parametres\nfrom sklearn.model_selection import GridSearchCV # test toutes les combinaisons","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Search to obtains the best RandomForestRegressor params"},{"metadata":{"trusted":true},"cell_type":"code","source":"# thi part take to much time, so I comment it\n# rfr_gcv = RandomForestRegressor()\n# param_distributions = {\n#     'n_estimators' = [10, 100, 200, 300]\n#     'min_samples_leaf' = [1, 5, 10]\n#     'min_samples_split' = [2, 10, 15]\n#     'max_depth' = [10, 40, 80, 90]\n# }\n\n# rs = GridSearchCV(rfr_gcv, param_distributions, scoring='neg_mean_squared_log_error')\n# rs.fit(X_train, y_train)\n# rs.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets estimate the score"},{"metadata":{"trusted":true},"cell_type":"code","source":"# rfr = RandomForestRegressor(n_estimators=19, min_samples_leaf=10, \n#                             min_samples_split=15, max_features='auto', max_depth=80, bootstrap=True)\n\n# RandomForestRegressor with best_params\n# rfr = RandomForestRegressor(n_estimators=300, min_samples_leaf=10, min_samples_split=15, \n#                             max_features='auto', max_depth=90, bootstrap=True) # that take too long time :/\n\nrfr = RandomForestRegressor(n_estimators=30, min_samples_leaf=10, min_samples_split=15, \n                            max_features='auto', max_depth=90, bootstrap=True)\n\ns_split = ShuffleSplit(n_splits=4, train_size=.12, test_size=.6) # allows to test on less data, so the cross validation takes less time\n\n# I commented this line to improve kernel execution time\n# np.sqrt(-cross_val_score(rfr, X_train, y_train, cv=s_split, scoring='neg_mean_squared_log_error', n_jobs=-1)).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's train the model with training values"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's predict the test trip_duration values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_copy = df_test.copy()\ndf_test_copy = create_datetime_based_columns(df_test_copy)\n# tmp_df_test = normalize_store_and_fwd_flag(tmp_df_test)\nX_test = filter_feature_columns(df_test_copy)\n\ny_test_pred = rfr.predict(X_test)\n# \nsubmission = pd.DataFrame({'id': df_test.index, 'trip_duration': y_test_pred})\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\n!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}